{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudipta-traya/langgraph/blob/main/docs/docs/examples/llm/bedrock_converse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1ca9ac",
      "metadata": {
        "id": "6d1ca9ac"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/bedrock_converse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3a8796-edc8-43f2-94ad-fe4fb20d70ed",
      "metadata": {
        "id": "9e3a8796-edc8-43f2-94ad-fe4fb20d70ed"
      },
      "source": [
        "# Bedrock Converse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b007403c-6b7a-420c-92f1-4171d05ed9bb",
      "metadata": {
        "id": "b007403c-6b7a-420c-92f1-4171d05ed9bb"
      },
      "source": [
        "## Basic Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ead155e-b8bd-46f9-ab9b-28fc009361dd",
      "metadata": {
        "id": "8ead155e-b8bd-46f9-ab9b-28fc009361dd"
      },
      "source": [
        "#### Call `complete` with a prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211db735",
      "metadata": {
        "id": "211db735"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f250bb49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f250bb49",
        "outputId": "fa772fb3-1422-4cbd-8cdc-c4e68454c1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-bedrock-converse\n",
            "  Downloading llama_index_llms_bedrock_converse-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting aioboto3<14.0.0,>=13.1.1 (from llama-index-llms-bedrock-converse)\n",
            "  Downloading aioboto3-13.4.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.122 (from llama-index-llms-bedrock-converse)\n",
            "  Downloading boto3-1.37.35-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.0 (from llama-index-llms-bedrock-converse)\n",
            "  Downloading llama_index_core-0.12.31-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting aiobotocore==2.18.0 (from aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse)\n",
            "  Downloading aiobotocore-2.18.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting aiofiles>=23.2.1 (from aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (3.11.15)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.36.2,>=1.36.0 (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse)\n",
            "  Downloading botocore-1.36.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (6.4.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (2.3.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (1.17.2)\n",
            "Collecting boto3<2.0.0,>=1.34.122 (from llama-index-llms-bedrock-converse)\n",
            "  Downloading boto3-1.36.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3<2.0.0,>=1.34.122->llama-index-llms-bedrock-converse)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2.0.40)\n",
            "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (9.1.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (4.13.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (1.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (1.19.0)\n",
            "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading griffe-1.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (4.3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (2025.1.31)\n",
            "INFO: pip is looking at multiple versions of s3transfer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3<2.0.0,>=1.34.122->llama-index-llms-bedrock-converse)\n",
            "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-llms-bedrock-converse) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (1.3.1)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-bedrock-converse) (3.0.2)\n",
            "Downloading llama_index_llms_bedrock_converse-0.4.15-py3-none-any.whl (13 kB)\n",
            "Downloading aioboto3-13.4.0-py3-none-any.whl (34 kB)\n",
            "Downloading aiobotocore-2.18.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.36.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_core-0.12.31-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.7/808.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading banks-2.1.1-py3-none-any.whl (28 kB)\n",
            "Downloading botocore-1.36.1-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading griffe-1.7.2-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: filetype, dirtyjson, mypy-extensions, marshmallow, jmespath, colorama, aioitertools, aiofiles, typing-inspect, tiktoken, griffe, botocore, s3transfer, dataclasses-json, banks, aiobotocore, llama-index-core, boto3, aioboto3, llama-index-llms-bedrock-converse\n",
            "Successfully installed aioboto3-13.4.0 aiobotocore-2.18.0 aiofiles-24.1.0 aioitertools-0.12.0 banks-2.1.1 boto3-1.36.1 botocore-1.36.1 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.2 jmespath-1.0.1 llama-index-core-0.12.31 llama-index-llms-bedrock-converse-0.4.15 marshmallow-3.26.1 mypy-extensions-1.0.0 s3transfer-0.11.3 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-bedrock-converse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "81732d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81732d83",
        "outputId": "58eea5c3-5557-4022-d99e-b820d5617542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.31-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.31 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.31)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.37-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.72.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.31->llama-index) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (3.11.15)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (4.13.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.31->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.18-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.12-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.19.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.31->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.31->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.12 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.12-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.31->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.31->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.31-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.37-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.1.18-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.12-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_cloud_services-0.6.12-py3-none-any.whl (36 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, python-dotenv, pypdf, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.1.18 llama-cloud-services-0.6.12 llama-index-0.12.31 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.37 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.12 pypdf-5.4.0 python-dotenv-1.1.0 striprtf-0.0.26\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# Load .env\n",
        "load_dotenv()\n",
        "\n",
        "# No need for profile_name if env vars are loaded\n",
        "resp = BedrockConverse(\n",
        "    # model=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ").complete(\"Paul Graham is \")\n",
        "\n",
        "print(resp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHn4oENtc5AL",
        "outputId": "4fb7cd40-138f-4674-e27a-efb29b30d156"
      },
      "id": "dHn4oENtc5AL",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a computer programmer, entrepreneur, venture capitalist, and essayist. He is best known for:\n",
            "\n",
            "1. Co-founding Viaweb (later sold to Yahoo! and became Yahoo! Store)\n",
            "2. Creating Y Combinator, one of the most successful startup accelerators\n",
            "3. Developing the programming language Arc\n",
            "4. Writing influential essays on technology, startups, and programming\n",
            "5. Popularizing the concept of \"Maker's Schedule, Manager's Schedule\"\n",
            "\n",
            "He was born in 1964 and received his PhD in Computer Science from Harvard. Graham is particularly influential in the startup community and has helped launch many successful companies through Y Combinator, including Dropbox, Airbnb, and Reddit. His essays on topics ranging from programming to entrepreneurship are widely read and respected in the tech industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "60be18ae-c957-4ac2-a58a-0652e18ee6d6",
      "metadata": {
        "id": "60be18ae-c957-4ac2-a58a-0652e18ee6d6"
      },
      "outputs": [],
      "source": [
        "# from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# profile_name = \"Your aws profile name\"\n",
        "# resp = BedrockConverse(\n",
        "#     model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "#     profile_name=profile_name,\n",
        "# ).complete(\"Paul Graham is \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ac2cbebb-a444-4a46-9d85-b265a3483d68",
      "metadata": {
        "id": "ac2cbebb-a444-4a46-9d85-b265a3483d68"
      },
      "outputs": [],
      "source": [
        "# print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14831268-f90f-499d-9d86-925dbc88292b",
      "metadata": {
        "id": "14831268-f90f-499d-9d86-925dbc88292b"
      },
      "source": [
        "#### Call `chat` with a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bbe29574-4af1-48d5-9739-f60652b6ce6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe29574-4af1-48d5-9739-f60652b6ce6c",
        "outputId": "1b8d4456-d621-4266-e558-662ee5346be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: *Adjusts tricorn hat and clears throat theatrically*\n",
            "\n",
            "Arr, gather 'round ye scurvy dogs, for I've got a tale that'll shiver yer timbers! \n",
            "\n",
            "'Twas a moonless night off the coast of Tortuga when me and me crew spotted a mysterious glow in the waters below. The sea was as black as a kraken's heart, but this ethereal light danced beneath the waves like ghostly fire.\n",
            "\n",
            "Me first mate, One-Eyed Pete, he says to me, \"Cap'n, we ought to steer clear!\" But ye know me - curiosity's always been me weakness, along with a good bottle of rum! *winks*\n",
            "\n",
            "As we drew closer, what should emerge from the depths but a massive sea turtle, glowing like a lighthouse beacon! But this were no ordinary turtle, mind ye - its shell was encrusted with gems that would make any pirate's treasure chest look like a collection of pebbles!\n",
            "\n",
            "The crew and I, we tried to catch the magnificent beast, but every time we got close, it would dive just out of reach, leading us through treacherous reefs and finally to a hidden cove we'd never seen before. And there, in that secret bay, we found the remains of an old Spanish galleon, still loaded with more gold than you could spend in ten lifetimes!\n",
            "\n",
            "*Takes a swig from flask*\n",
            "\n",
            "Some say that turtle was the spirit of an old sea captain, leading worthy sailors to their fortune. Others reckon it was just good luck. But I'll tell ye this - we never saw that glowing turtle again, though every now and then, on particularly dark nights, I swear I see a familiar glow beneath the waves...\n",
            "\n",
            "YARR! And that's the truth, or I'm not Captain Salty McBarnacle! *adjusts peg leg*\n",
            "\n",
            "Now, who's buying the next round? *grins mischievously*\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
        "]\n",
        "\n",
        "\n",
        "resp = BedrockConverse(\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ").chat(messages)\n",
        "\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbd550a-0264-4a11-9b2c-a08d8723a5ae",
      "metadata": {
        "id": "9cbd550a-0264-4a11-9b2c-a08d8723a5ae"
      },
      "outputs": [],
      "source": [
        "# print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed5e894-4597-4911-a623-591560f72b82",
      "metadata": {
        "id": "2ed5e894-4597-4911-a623-591560f72b82"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb7986f-aaed-42e2-abdd-f274f6d4fc59",
      "metadata": {
        "id": "4cb7986f-aaed-42e2-abdd-f274f6d4fc59"
      },
      "source": [
        "Using `stream_complete` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d43f17a2-0aeb-464b-a7a7-732ba5e8ef24",
      "metadata": {
        "id": "d43f17a2-0aeb-464b-a7a7-732ba5e8ef24"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "resp = llm.stream_complete(\"Paul Graham is \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0214e911-cf0d-489c-bc48-9bb1d8bf65d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0214e911-cf0d-489c-bc48-9bb1d8bf65d8",
        "outputId": "b4e62c43-ab30-4ade-85dd-c08bd66d3d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a computer programmer, entrepreneur, and venture capitalist. He's best known for:\n",
            "\n",
            "1. Co-founding Y Combinator, one of the most influential startup accelerators in Silicon Valley\n",
            "\n",
            "2. Creating Viaweb, one of the first web-based application platforms (later sold to Yahoo! and became Yahoo! Store)\n",
            "\n",
            "3. Developing the programming language Arc\n",
            "\n",
            "4. Writing influential essays about technology, startups, and programming on his website (paulgraham.com)\n",
            "\n",
            "5. Co-founding Hacker News, a popular tech news aggregator site\n",
            "\n",
            "He's also known for his thought leadership in the startup world and his advocacy for the Lisp programming language. Graham received his Ph.D. in Computer Science from Harvard and studied painting at the Accademia di Belle Arti in Florence."
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40350dd8-3f50-4a2f-8545-5723942039bb",
      "metadata": {
        "id": "40350dd8-3f50-4a2f-8545-5723942039bb"
      },
      "source": [
        "Using `stream_chat` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc636e65-a67b-4dcd-ac60-b25abc9d8dbd",
      "metadata": {
        "id": "bc636e65-a67b-4dcd-ac60-b25abc9d8dbd"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
        "]\n",
        "resp = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4475a6bc-1051-4287-abce-ba83324aeb9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4475a6bc-1051-4287-abce-ba83324aeb9e",
        "outputId": "278c87b9-5197-4875-a183-fa257761a4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yarr, gather 'round ye scurvy dogs, for I've got a tale that'll shiver yer timbers! \n",
            "\n",
            "'Twas a moonless night off the coast of Tortuga when me crew and I spotted a mysterious glow in the distance. The sea was as black as a kraken's heart, but this eerie light danced across the waves like ghost fire.\n",
            "\n",
            "Me first mate, One-Eyed Pete, swore it was a siren's trap, but I knew better. We steered the ship toward that ghostly beacon, and what did we find? A Spanish galleon, abandoned and adrift! But here's the peculiar part - every inch of the ship was covered in glowing blue moss that lit up the night like fairy lights.\n",
            "\n",
            "The crew was hesitant to board her, but old Captain Blackhook (that's me) never backed down from adventure! When we swung aboard, we found the ship's log, written in trembling hand, speaking of a curse from a sea witch who'd been denied her share of treasure.\n",
            "\n",
            "But curse or no curse, we weren't leaving empty-handed! We found a chest full of emeralds green as the Caribbean waters. Though I'll tell ye true - to this day, those gems glow in the dark, and sometimes I swear I can hear them singing sea shanties at night!\n",
            "\n",
            "*takes a swig from rum bottle*\n",
            "\n",
            "And that's the honest truth, or I'm not Captain Blackhook! Though some say the rum might've had something to do with the singing gems... HARR HARR HARR! 🏴‍☠️"
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009d3f1c-ef35-4126-ae82-0b97adb746e3",
      "metadata": {
        "id": "009d3f1c-ef35-4126-ae82-0b97adb746e3"
      },
      "source": [
        "## Configure Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e973e3d1-a3c9-43b9-bee1-af3e57946ac3",
      "metadata": {
        "id": "e973e3d1-a3c9-43b9-bee1-af3e57946ac3"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e2c9bcf6-c950-4dfc-abdc-598d5bdedf40",
      "metadata": {
        "id": "e2c9bcf6-c950-4dfc-abdc-598d5bdedf40"
      },
      "outputs": [],
      "source": [
        "resp = llm.complete(\"Paul Graham is \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "de715fea-4878-4fbb-b415-71250215f3a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de715fea-4878-4fbb-b415-71250215f3a6",
        "outputId": "b721897c-0742-488a-fb59-a87a1eefbdc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a computer programmer, entrepreneur, and essayist. He's best known for:\n",
            "\n",
            "1. Co-founding Y Combinator, one of the most influential startup accelerators in Silicon Valley\n",
            "\n",
            "2. Creating Viaweb (with Robert Morris), which was one of the first web-based application platforms and was later sold to Yahoo! in 1998\n",
            "\n",
            "3. Writing influential essays on technology, startups, and programming on his website (paulgraham.com)\n",
            "\n",
            "4. Developing the programming language Arc\n",
            "\n",
            "5. Being an advocate for Lisp programming language\n",
            "\n",
            "6. Writing books including \"Hackers & Painters\" and \"On Lisp\"\n",
            "\n",
            "He's particularly influential in the startup world, having helped launch many successful companies through Y Combinator, including Dropbox, Airbnb, and Reddit. Graham is known for his thoughtful essays on entrepreneurship, technology, and society, which have become required reading for many in the tech industry.\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bdd4602-e37c-4230-af82-35af5292f9a0",
      "metadata": {
        "id": "1bdd4602-e37c-4230-af82-35af5292f9a0"
      },
      "source": [
        "## Connect to Bedrock with Access Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a9c80814-6d59-4782-a4bb-cbfcdba6a072",
      "metadata": {
        "id": "a9c80814-6d59-4782-a4bb-cbfcdba6a072"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Paul Graham is \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a07982f4-035c-41ca-9dca-49ae6ab3c05a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07982f4-035c-41ca-9dca-49ae6ab3c05a",
        "outputId": "7e71c3e6-9167-4298-d293-16eecb8c9904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a computer programmer, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
            "\n",
            "1. Co-founding Y Combinator, one of the most influential startup accelerators in Silicon Valley\n",
            "\n",
            "2. Creating Viaweb, one of the first web-based application platforms (later sold to Yahoo! and became Yahoo! Store)\n",
            "\n",
            "3. Developing the programming language Arc\n",
            "\n",
            "4. Writing influential essays on technology, startups, and programming on his website (paulgraham.com)\n",
            "\n",
            "5. Co-founding Hacker News, a popular tech news aggregator site\n",
            "\n",
            "He's particularly respected for his insights into startups and technology, and his essays have become required reading for many entrepreneurs and programmers. Graham received his Ph.D. in Computer Science from Harvard and studied painting at RISD and the Accademia di Belle Arti in Florence.\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73eb402",
      "metadata": {
        "id": "a73eb402"
      },
      "source": [
        "## Function Calling\n",
        "\n",
        "Claude, Command and Mistral Large models supports native function calling through AWS Bedrock Converse. There's a seamless integration with LlamaIndex tools, through the `predict_and_call` function on the `llm`.\n",
        "\n",
        "This allows the user to attach any tools and let the LLM decide which tools to call (if any).\n",
        "\n",
        "If you wish to perform tool calling as part of an agentic loop, check out our [agent guides](https://docs.llamaindex.ai/en/latest/module_guides/deploying/agents/) instead.\n",
        "\n",
        "**NOTE**: Not all models from AWS Bedrock support function calling and the Converse API. [Check the available features of each LLM here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-features.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "427105db",
      "metadata": {
        "id": "427105db"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def mystery(a: int, b: int) -> int:\n",
        "    \"\"\"Mystery function on two integers.\"\"\"\n",
        "    return a * b + a + b\n",
        "\n",
        "\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    # model=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
        "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    # model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d5a2f465",
      "metadata": {
        "id": "d5a2f465"
      },
      "outputs": [],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [mystery_tool, multiply_tool],\n",
        "    user_msg=\"What happens if I run the mystery function on 5 and 7\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "41313d40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41313d40",
        "outputId": "204721d2-aca9-4df6-94c8-a7628fb0a742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "37b5d62f",
      "metadata": {
        "id": "37b5d62f"
      },
      "outputs": [],
      "source": [
        "response = llm.predict_and_call(\n",
        "    [mystery_tool, multiply_tool],\n",
        "    user_msg=(\n",
        "        \"\"\"What happens if I run the mystery function on the following pairs of numbers? Generate a separate result for each row:\n",
        "- 1 and 2\n",
        "- 8 and 4\n",
        "- 100 and 20\n",
        "\n",
        "NOTE: you need to run the mystery function for all of the pairs above at the same time \\\n",
        "\"\"\"\n",
        "    ),\n",
        "    allow_parallel_tool_calls=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0a8295e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8295e0",
        "outputId": "e795bd04-bb9c-4bef-80a8-6f24c9288afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "\n",
            "44\n",
            "\n",
            "2120\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "45b6081a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45b6081a",
        "outputId": "7b3832f5-7aac-40e9-ed9a-9f7eb1468198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: mystery, Input: {'args': (), 'kwargs': {'a': 1, 'b': 2}}, Output: 5\n",
            "Name: mystery, Input: {'args': (), 'kwargs': {'a': 8, 'b': 4}}, Output: 44\n",
            "Name: mystery, Input: {'args': (), 'kwargs': {'a': 100, 'b': 20}}, Output: 2120\n"
          ]
        }
      ],
      "source": [
        "for s in response.sources:\n",
        "    print(f\"Name: {s.tool_name}, Input: {s.raw_input}, Output: {str(s)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bee3bc",
      "metadata": {
        "id": "a7bee3bc"
      },
      "source": [
        "## Async"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8f4c144f",
      "metadata": {
        "id": "8f4c144f"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# llm = BedrockConverse(\n",
        "#     model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "#     aws_access_key_id=\"AWS Access Key ID to use\",\n",
        "#     aws_secret_access_key=\"AWS Secret Access Key to use\",\n",
        "#     aws_session_token=\"AWS Session Token to use\",\n",
        "#     region_name=\"AWS Region to use, eg. us-east-1\",\n",
        "# )\n",
        "\n",
        "llm = BedrockConverse(\n",
        "    # model=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
        "    # model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "\n",
        "resp = await llm.acomplete(\"Paul Graham is \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "cd72e3a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd72e3a0",
        "outputId": "68e6336b-1be2-45f9-e7e1-3d767a9a49b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a computer programmer, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
            "\n",
            "1. Co-founding Viaweb (later sold to Yahoo! and became Yahoo! Store)\n",
            "2. Creating Y Combinator, one of the most influential startup accelerators\n",
            "3. Developing the Lisp programming language dialect called Arc\n",
            "4. Writing influential essays on technology, startups, and programming\n",
            "5. His work on spam filtering and Bayesian spam filters\n",
            "\n",
            "He received his PhD in Computer Science from Harvard and has been a significant influence in the startup and tech community since the 1990s. Graham is particularly known for his thoughtful essays on entrepreneurship, programming, and technology which are widely read in the tech industry.\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# Initialize the Bedrock LLM client\n",
        "llm = BedrockConverse(\n",
        "    # Example models you can swap:\n",
        "    # model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    # model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "\n",
        "    model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "\n",
        "# Async function to call the model\n",
        "async def run_mystery_function():\n",
        "    resp = await llm.acomplete(\n",
        "        tools=[mystery_tool, multiply_tool],\n",
        "        user_msg=(\n",
        "            \"\"\"What happens if I run the mystery function on the following pairs of numbers?\n",
        "Generate a separate result for each row:\n",
        "- 1 and 2\n",
        "- 8 and 4\n",
        "- 100 and 20\n",
        "\n",
        "NOTE: you need to run the mystery function for all of the pairs above at the same time.\"\"\"\n",
        "        ),\n",
        "        allow_parallel_tool_calls=True,\n",
        "    )\n",
        "\n",
        "    return resp\n",
        "\n",
        "# To run this in an async environment (like Jupyter or an event loop):\n",
        "# import asyncio\n",
        "# result = asyncio.run(run_mystery_function())\n",
        "# print(result)\n"
      ],
      "metadata": {
        "id": "-jl3zgIhfpek"
      },
      "id": "-jl3zgIhfpek",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove this line\n",
        "#import asyncio\n",
        "\n",
        "# Modify this line\n",
        "result = await run_mystery_function()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "_MkJgen4gkhn",
        "outputId": "e741c2cd-4199-44db-d9af-00178cb5bf01"
      },
      "id": "_MkJgen4gkhn",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No prompt provided in positional or keyword arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-aa165a5ed837>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Modify this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrun_mystery_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9f116c50ad7d>\u001b[0m in \u001b[0;36mrun_mystery_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Async function to call the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_mystery_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     resp = await llm.acomplete(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmystery_tool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiply_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         user_msg=(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mwrapped_async_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0m_self\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         ) -> Any:\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             with wrapper_logic(_self) as callback_manager, callback_manager.as_trace(\n\u001b[1;32m    311\u001b[0m                 \u001b[0;34m\"completion\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mextract_prompt\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    303\u001b[0m                     \u001b[0;34m\"No prompt provided in positional or keyword arguments\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: No prompt provided in positional or keyword arguments"
          ]
        }
      ]
    },
    {
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# Initialize the Bedrock LLM client\n",
        "llm = BedrockConverse(\n",
        "    # Example models you can swap:\n",
        "    # model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    model=\"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        "    # model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "\n",
        "    # model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "\n",
        "# Async function to call the model\n",
        "async def run_mystery_function():\n",
        "    # The acomplete method expects a 'prompt' argument\n",
        "    resp = await llm.acomplete(\n",
        "        # Pass the user_msg as the prompt\n",
        "        prompt=(\n",
        "            \"\"\"What happens if I run the mystery function on the following pairs of numbers?\n",
        "Generate a separate result for each row:\n",
        "- 1 and 2\n",
        "- 8 and 4\n",
        "- 100 and 20\n",
        "\n",
        "NOTE: you need to run the mystery function for all of the pairs above at the same time.\"\"\"\n",
        "        ),\n",
        "        tools=[mystery_tool, multiply_tool],\n",
        "        # user_msg is not needed here\n",
        "        # user_msg=(\n",
        "        #     \"\"\"What happens if I run the mystery function on the following pairs of numbers?\n",
        "        # Generate a separate result for each row:\n",
        "        # - 1 and 2\n",
        "        # - 8 and 4\n",
        "        # - 100 and 20\n",
        "\n",
        "        # NOTE: you need to run the mystery function for all of the pairs above at the same time.\"\"\"\n",
        "        # ),\n",
        "        allow_parallel_tool_calls=True,\n",
        "    )\n",
        "\n",
        "    return resp\n",
        "\n",
        "# To run this in an async environment (like Jupyter or an event loop):\n",
        "# import asyncio\n",
        "# result = asyncio.run(run_mystery_function())\n",
        "# print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IOt5eW7fgx5-"
      },
      "id": "IOt5eW7fgx5-",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call(\n",
        "    [mystery_tool, multiply_tool],\n",
        "    user_msg=(\n",
        "        \"\"\"What happens if I run the mystery function on the following pairs of numbers? Generate a separate result for each row:\n",
        "- 1 and 2\n",
        "- 8 and 4\n",
        "- 100 and 20\n",
        "\n",
        "NOTE: you need to run the mystery function for all of the pairs above at the same time \\\n",
        "\"\"\"\n",
        "    ),\n",
        "    allow_parallel_tool_calls=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3anW2c1tf9sm"
      },
      "id": "3anW2c1tf9sm",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "# Initialize the Bedrock LLM client\n",
        "llm = BedrockConverse(\n",
        "    # Replace with the ARN of your inference profile\n",
        "     model=\"apac.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
        "    # model=\"arn:aws:bedrock:ap-south-1:965119681473:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
        ")\n",
        "\n",
        "response = llm.predict_and_call(\n",
        "    [mystery_tool, multiply_tool],\n",
        "    user_msg=(\n",
        "        \"\"\"What happens if I run the mystery function on the following pairs of numbers? Generate a separate result for each row:\n",
        "- 1 and 2\n",
        "- 8 and 4\n",
        "- 100 and 20\n",
        "\n",
        "NOTE: you need to run the mystery function for all of the pairs above at the same time \\\n",
        "\"\"\"\n",
        "    ),\n",
        "    allow_parallel_tool_calls=True,\n",
        ")"
      ],
      "metadata": {
        "id": "fM0Ne-eNh1Sf"
      },
      "id": "fM0Ne-eNh1Sf",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmxpzII8gzZa",
        "outputId": "e28b6ddd-793d-4a48-dd37-8d689355d277"
      },
      "id": "bmxpzII8gzZa",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='5\\n\\n44\\n\\n2120', sources=[ToolOutput(content='5', tool_name='mystery', raw_input={'args': (), 'kwargs': {'a': 1, 'b': 2}}, raw_output=5, is_error=False), ToolOutput(content='44', tool_name='mystery', raw_input={'args': (), 'kwargs': {'a': 8, 'b': 4}}, raw_output=44, is_error=False), ToolOutput(content='2120', tool_name='mystery', raw_input={'args': (), 'kwargs': {'a': 100, 'b': 20}}, raw_output=2120, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmG0Ff3mg6c2"
      },
      "id": "RmG0Ff3mg6c2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}